{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNAqv2e9+/Z53fxKBH/k7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t8101349/LLM-Finetune/blob/main/unsloth_Gemma3_%E5%BE%AE%E8%AA%BF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w7TRkNK6QEi7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 从 unsloth 库中导入 FastModel 类，用于快速加载和操作预训练模型\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "# 使用 FastModel.from_pretrained 方法加载预训练模型和对应的分词器\n",
        "# 该方法返回一个元组 (model, tokenizer)，分别代表模型和分词器对象\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "\n",
        "    max_seq_length = 2048,  #  设置模型支持的最大序列长度，按需调整\n",
        "    load_in_4bit = True,    # 是否启用 4 位量化，使用 4 位量化加载模型，可以显著减少内存占用，但可能会略微牺牲精度\n",
        "    load_in_8bit = False,   #  8 位量化相比 4 位更精确，但内存占用要翻番，如果需要更高精度且内存充足，可以设置为 True\n",
        "    full_finetuning = False, # 全参数微调\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "k8FKZms_QMC8",
        "outputId": "096f5aaf-7fef-4da3-aa5a-50bb6f7120cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c1038ba6c827>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 从 unsloth 库中导入 FastModel 类，用于快速加载和操作预训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 使用 FastModel.from_pretrained 方法加载预训练模型和对应的分词器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Fix Xformers performance issues since 0.0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # 纯文本任务，关闭视觉层的微调，以节省资源\n",
        "    finetune_language_layers   = True,  # 微调模型中的语言相关层，语言层是文本任务的核心部分，要开\n",
        "    finetune_attention_modules = True,  #微调模型中的注意力模块\n",
        "    finetune_mlp_modules       = True,  # 微调模型中的MLP模块\n",
        "    r = 8,           # LoRA中的秩，秩越高，模型表达能力越强，训练越慢，也越容易过拟合\n",
        "    lora_alpha = 8,  # LoRA中的缩放因子\n",
        "    lora_dropout = 0,  #LoRA中的dropout率\n",
        "    bias = \"none\",\n",
        "    random_state = 11,  #随机数种子，这个随便取\n",
        ")"
      ],
      "metadata": {
        "id": "ftcibh1QQMLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# 加载数据集，使用 streaming=True 避免一次性下载全部数据\n",
        "dataset = load_dataset(\"Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT\", streaming=True)\n",
        "\n",
        "# 过滤数据，只保留 repo_name 为 'zhihu/zhihu_score9.0-10_clean_v10' 的数据\n",
        "filtered_dataset = dataset.filter(lambda example: example['repo_name'] == 'zhihu/zhihu_score9.0-10_clean_v10')\n",
        "\n",
        "\n",
        "\n",
        "# 将过滤后的数据保存为 JSON 文件\n",
        "def save_to_json(dataset, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for example in dataset:\n",
        "            #filtered_dataset是一个可迭代对象，需要逐条example提取\n",
        "            json.dump(example, f, ensure_ascii=False)\n",
        "            f.write('\\n')  # 每条数据之间添加换行符\n",
        "\n",
        "\n",
        "save_to_json(filtered_dataset['train'], 'zhihu_data.json')\n",
        "\n",
        "\n",
        "print(\"数据已保存到 xhs_data.json\")"
      ],
      "metadata": {
        "id": "hYzxqfXaQMTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 加载本地的 json 数据集，文件名为 zhihu_data.json\n",
        "dataset = load_dataset(\"json\", data_files=\"zhihu_data.json\", split=\"train\")\n",
        "\n",
        "def apply_chat_template_zhihu(examples):\n",
        "    conversations = []\n",
        "    for instruction, input_text, output_text in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
        "        # 构建用户提问部分：若 input 非空则合并，否则仅使用 instruction\n",
        "        if input_text.strip():\n",
        "            user_message = instruction.strip() + \"\\n\" + input_text.strip()\n",
        "        else:\n",
        "            user_message = instruction.strip()\n",
        "\n",
        "        # 去除 output 中 <think> 和 </think> 包裹的思考内容\n",
        "        cleaned_output = re.sub(r'<think>.*?</think>', '', output_text, flags=re.DOTALL).strip()\n",
        "\n",
        "        # 构造最终对话格式\n",
        "        conversation = (\n",
        "            \"<bos>\\n\"\n",
        "            \"<start_of_turn>user\\n\" + user_message + \"\\n<end_of_turn>\\n\"\n",
        "            \"<start_of_turn>model\\n\" + cleaned_output + \"\\n<end_of_turn>\"\n",
        "        )\n",
        "        conversations.append(conversation)\n",
        "    return {\"text\": conversations}\n",
        "\n",
        "# 对数据集应用转换函数\n",
        "dataset = dataset.map(apply_chat_template_zhihu, batched=True)"
      ],
      "metadata": {
        "id": "kpbXXMHwQMaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # 不进行评估，可以替换为具体数据集以启用评估\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # 梯度累积步数\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4, #学习率\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\", #优化器，是 AdamW 优化器的 8 位版本，节省内存且适合大模型。\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 114514,\n",
        "        report_to = \"none\", # 日志输出目标，这里表示不使用外部工具（如 WandB）记录日志\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "XRbmaVAwQMi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "3xEiAQP4QMqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "BIX4eWjlQM0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\n",
        "        \"type\" : \"text\",\n",
        "        \"text\" : \"神经网络的参数都是随机的，有的效果很好，有的效果很差，这真的不是玄学吗？\",\n",
        "    }]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        ")\n",
        "outputs = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 1024,\n",
        "\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        ")\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "C9J1-tBMQM7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#流式輸出\n",
        "\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"我亲爸和我亲妈结婚，算不算近亲结婚？\",}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256,\n",
        "\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ],
      "metadata": {
        "id": "ji6nZAUpQM_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}